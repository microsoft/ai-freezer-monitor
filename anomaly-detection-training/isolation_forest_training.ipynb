{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37164bitpearsonpytorchcondade033092a7bd4811a1b3d0692796d612",
   "display_name": "Python 3.7.1 64-bit ('Pearson-PyTorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest\n",
    "\n",
    "In this Jupyter notebook, you'll use the data you collected with your IoT Freezer Monitor to train a machine learning model to detect anomalies in the operation of your freezer. The goal of using a anomaly detection is to send you a warning before freezer has an obvious malfunction.\n",
    "\n",
    "## What is an Isolation Forest\n",
    "\n",
    "Isolation Forest is a machine learning algorithem that is used to identify outliers in unlabeled data. Unlabeled data mean that you don't know whether your training data set contains anomalies. The data you're using is most likely unlabeled since hopefully your freezer didn't break while you recording the temperature. \n",
    "\n",
    "## How does this work?\n",
    "\n",
    "The isolation forest looks for outliers by randomly spliting the data recursivly until a point is isolated from all other point, the fewer steps it needs to do this the more likely the point is an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment\n",
    "\n",
    "The sections below prepares the Python environment with all the libraries you need to train you anomaly detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process your raw data\n",
    "\n",
    "The following sections will load your data set, trim any unwanted data out, and split the data into individual timed increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def hours_to_seconds(hours):\n",
    "    return hours * 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caluclate how many samples to include per file\n",
    "# The sample rate should match the code on your temperature monitor\n",
    "sample_rate = 0.2   # Hz\n",
    "sample_time = .25     # Hours\n",
    "samples_per_file = sample_rate * hours_to_seconds(sample_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the dataset from Adafruit.io\n",
    "# Change <DATASET-NAME> to the file name of the Adafruit.io data you downloaded.\n",
    "dataset_path = './dataset/raw/<DATASET-NAME>'\n",
    "\n",
    "# Load the dataset using pandas\n",
    "df = pd.read_csv(dataset_path, usecols=[1])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.plot(df)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You may want to trim start and end of your data. Espcially if the data at the start of the set is from outside the freezer or while the thermocouple was cooling down.\n",
    "\n",
    "start_time_trim = 1 # Hours\n",
    "end_time_trim = 1   # Hours\n",
    "start_time = hours_to_seconds(start_time_trim)\n",
    "end_time = len(df.index) - hours_to_seconds(end_time_trim)\n",
    "\n",
    "df = df.truncate(before=start_time, after=end_time)\n",
    "\n",
    "# Create one file for each group of samples\n",
    "arr = []\n",
    "for i, temp in df.iterrows():\n",
    "    # starting_idx = i\n",
    "    arr.append(temp)\n",
    "    if i % samples_per_file == 0 and i != start_time:\n",
    "        sample = pd.DataFrame(data=arr)\n",
    "        sample.to_csv('./dataset/training/output_'+str(i), index=False, header=False)\n",
    "        arr = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare you data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of file names in directory\n",
    "samples_in_dir = listdir('./dataset/training')\n",
    "# Join the path and file name for all files in dicrectory\n",
    "samples_in_dir = [join('./dataset/training', sample) for sample in samples_in_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the size of the validation set to 20%\n",
    "val_size = int(.2 * len(samples_in_dir))\n",
    "\n",
    "# Randomize the samples\n",
    "np.random.shuffle(samples_in_dir)\n",
    "\n",
    "# Split data into training samples and validation samples\n",
    "val_samples = samples_in_dir[:val_size]\n",
    "train_samples = samples_in_dir[val_size:]\n",
    "\n",
    "# Check that the data split correctly\n",
    "assert(len(val_samples) + len(train_samples) == len(samples_in_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the data set loaded correctly\n",
    "np.loadtxt(samples_in_dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sample):\n",
    "    features = []\n",
    "\n",
    "    # Median absolute deviation (MAD)\n",
    "    mad = sp.stats.median_absolute_deviation(sample)\n",
    "\n",
    "    features.append(mad)\n",
    "    return np.array(features).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load 1 sample to test feature extraction\n",
    "sample = np.loadtxt(samples_in_dir[0])\n",
    "mean = np.mean(sample)\n",
    "features = extract_features(sample)\n",
    "print(sample.shape)\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: loop through filenames, creating feature sets\n",
    "def create_feature_set(filenames):\n",
    "    x_out = []\n",
    "    for file in filenames:\n",
    "        sample = np.loadtxt(file)\n",
    "        features = extract_features(sample)\n",
    "        x_out.append(features)\n",
    "    return np.array(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract Features for the training and validation sets\n",
    "training = create_feature_set(train_samples)\n",
    "val = create_feature_set(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iforest start\n",
    "\n",
    "clf = IsolationForest(max_samples=100, n_estimators=100, contamination=0.03)\n",
    "clf.fit(training)\n",
    "y_pred_train = clf.predict(training)\n",
    "y_pred_val = clf.predict(val)\n",
    "\n",
    "print(\"Training Accuracy:\", list(y_pred_train).count(1)/y_pred_train.shape[0])\n",
    "print(\"Validation Accuracy:\", list(y_pred_val).count(1)/y_pred_val.shape[0])"
   ]
  }
 ]
}