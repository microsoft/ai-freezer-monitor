{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599782212675",
   "display_name": "Python 3.8.2 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Training Notebook\n",
    "In this Jupyter notebook you'll process the data you collected from your freezer and use to train an autoencoder machine learning model that will be able to detect anomalies in the operation of your freezer. Going through this notebook should take approximatly 30 minutes and it's best to do it in one sitting. We use Python and a few machine learning specific libraries, you won't have to write any code, just follow the steps and run the code blocks. If you're not familiar with Jupyter notebooks here is a useful tutorial for getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download your data\n",
    "First you'll need to download your temperature data from Adafruit.io. Adafruit put together a great guide on how to do this [here](https://learn.adafruit.com/adafruit-io-basics-feeds/downloading-feed-data). Make sure to download the data as a CSV and move into the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        -19.5625\n0       -19.6250\n1       -19.6250\n2       -19.6875\n3       -19.6875\n4       -19.7500\n...          ...\n228511  -18.2500\n228512  -18.2500\n228513  -18.2500\n228514  -18.1875\n228515  -18.1875\n\n[228516 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-19.5625</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-19.6250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-19.6250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-19.6875</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-19.6875</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-19.7500</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>228511</th>\n      <td>-18.2500</td>\n    </tr>\n    <tr>\n      <th>228512</th>\n      <td>-18.2500</td>\n    </tr>\n    <tr>\n      <th>228513</th>\n      <td>-18.2500</td>\n    </tr>\n    <tr>\n      <th>228514</th>\n      <td>-18.1875</td>\n    </tr>\n    <tr>\n      <th>228515</th>\n      <td>-18.1875</td>\n    </tr>\n  </tbody>\n</table>\n<p>228516 rows Ã— 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Path to the dataset from Adafruit.io\n",
    "# Change <PATH-TO-DATASET> to the file name of the Adafruit.io data you downloaded.\n",
    "dataset_path = './dataset/raw/known_good.csv'\n",
    "\n",
    "# Load the dataset using pandas\n",
    "df = pd.read_csv(dataset_path, usecols=[1])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing your data\n",
    "Next you'll restructure the data set so that each line represtened the time period you want to look at. An hour seems to be long enough to account for any normal irregualties in the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The sample rate should match the code on your temperature monitor\n",
    "\n",
    "def hours_to_seconds(hours):\n",
    "    return hours * 3600\n",
    "\n",
    "sample_rate = 0.2   # Hz (samples per second)\n",
    "sample_time = 1     # Hours\n",
    "datapoints_per_row = sample_rate * hours_to_seconds(sample_time)\n",
    "start_time = 0\n",
    "\n",
    "x_train = pd.DataFrame()\n",
    "arr = []\n",
    "for i, temp in df.iterrows():\n",
    "    if i % datapoints_per_row == 0 and i != start_time:\n",
    "        sample = pd.DataFrame(data=arr).T\n",
    "        x_train = x_train.append(sample, ignore_index=True)\n",
    "        arr = []\n",
    "    arr.append(df.values[i])\n",
    "raw_data = x_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you train a machine learning model you're going to want to make sure that it works before putting out in the world. To do that you reserve some amount of the data for testing after you train the model. You'll also use validation set during training, but we'll split that one out of the training data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    raw_data, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this data later to verify that your microconroller code is working correctly\n",
    "export_data = np.array(test_data)\n",
    "np.savetxt('arduino_test.csv', export_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueral networks tend to work best when the values of the data is on a similar scale. It's good practice to normalize your data, in this case you'll normalize your data to values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_val = tf.reduce_min(train_data)\n",
    "max_val = tf.reduce_max(train_data)\n",
    "\n",
    "train_data = (train_data - min_val) / (max_val - min_val)\n",
    "test_data = (test_data - min_val) / (max_val - min_val)\n",
    "\n",
    "train_data = tf.cast(train_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets graph a normal hour of temperature data. Try changing the index value of \"train_data\" to see how different parts of the dataset look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(np.arange(720), train_data[0])\n",
    "plt.title(\"Normal Temps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your model\n",
    "In this section you'll define the model architecture and train the model. Then you'll evaulate how the model is performing. We won't go into detial of how Tensor Flow works here, but there further reading the readme has great content about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector(Model):\n",
    "  def __init__(self):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(32, activation=\"relu\"),\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(8, activation=\"relu\")])\n",
    "    \n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(32, activation=\"relu\"),\n",
    "      layers.Dense(720, activation=\"sigmoid\")])\n",
    "    \n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = autoencoder.fit(train_data, train_data, \n",
    "          epochs=60, \n",
    "          batch_size=16,\n",
    "          validation_split = 0.1,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've trained you model you can look at the ending loss and validation loss, you want them both to be pretty small, less than 0.05, and close together in value. We an also see in the next graph how the model progressed in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(train_data).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.plot(train_data[0],'b')\n",
    "plt.plot(decoded_imgs[0],'r')\n",
    "plt.fill_between(np.arange(720), decoded_imgs[0], train_data[0], color='lightcoral' )\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(test_data).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.plot(test_data[0],'b')\n",
    "plt.plot(decoded_imgs[0],'r')\n",
    "plt.fill_between(np.arange(720), decoded_imgs[0], test_data[0], color='lightcoral' )\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(train_data)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, train_data)\n",
    "\n",
    "plt.hist(train_loss, bins=50)\n",
    "plt.xlabel(\"Train loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Finally we want to check how accurate the model is with data it's never seen before. First we'll print the metrics from the training set and then we can compare the metrics from the test set. If the metrics look close we know our model has generlized well enough to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "  reconstructions = model(data)\n",
    "  loss = tf.keras.losses.mae(reconstructions, data)\n",
    "  return tf.math.less(loss, threshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, preds)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, preds)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = predict(autoencoder, train_data, threshold)\n",
    "labels = np.ones(preds.shape[0])\n",
    "print_stats(preds, labels)\n",
    "\n",
    "preds = predict(autoencoder, test_data, threshold)\n",
    "labels = np.ones(preds.shape[0])\n",
    "print_stats(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('const int input_size = {};'.format(train_data.shape[1]))\n",
    "print('const float threshold = {};'.format(threshold))\n",
    "print('const float min_val = {};'.format(min_val))\n",
    "print('const float max_val = {};'.format(max_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the model to a TF Lite flat buffer.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(autoencoder)\n",
    "# We optimize the model in order to save room on the microcontroller. Optimizing the model will have a slight affect on the results, but for this use case it won't be a problem.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TF Lite model.\n",
    "with tf.io.gfile.GFile('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the anomaly dataset from Adafruit.io\n",
    "# Change <ANOMALY-DATASET-NAME> to the file name of the Adafruit.io data you downloaded.\n",
    "# dataset_path = './dataset/raw/known_anomaly.csv'\n",
    "dataset_path = './dataset/raw/sept_test.csv'\n",
    "\n",
    "# Load the dataset using pandas\n",
    "df_anomaly = pd.read_csv(dataset_path, usecols=[1])\n",
    "\n",
    "df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resort the data set into rows of 1 hour cycles\n",
    "start_time = 0\n",
    "samples_per_file = 720\n",
    "\n",
    "x_train = pd.DataFrame()\n",
    "arr = []\n",
    "for i, temp in df_anomaly.iterrows():\n",
    "    if i % datapoints_per_row == 0 and i != start_time:\n",
    "        sample = pd.DataFrame(data=arr).T\n",
    "        x_train = x_train.append(sample, ignore_index=True)\n",
    "        arr = []\n",
    "    arr.append(df_anomaly.values[i])\n",
    "anomaly_data = x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_data = (anomaly_data - min_val) / (max_val - min_val)\n",
    "\n",
    "anomaly_data = tf.cast(anomaly_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(anomaly_data).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.plot(anomaly_data[2],'b')\n",
    "plt.plot(decoded_imgs[2],'r')\n",
    "plt.fill_between(np.arange(720), decoded_imgs[2], anomaly_data[2], color='lightcoral' )\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = autoencoder.predict(test_data)\n",
    "test_loss = tf.keras.losses.mae(reconstructions, test_data)\n",
    "\n",
    "plt.hist(test_loss, bins=50)\n",
    "plt.xlabel(\"Test loss\")\n",
    "plt.ylabel(\"No of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = predict(autoencoder, anomaly_data, threshold)\n",
    "print_stats(preds, test_labels)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='./model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(train_data[1])\n",
    "# print([train_data[0]].shape)\n",
    "interpreter.set_tensor(input_details[0]['index'], [input_data])\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "loss = tf.keras.losses.mae(tflite_results, train_data[1])\n",
    "print(loss)"
   ]
  }
 ]
}